The fusion of computer vision and natural language processing has propelled advancements in artificial intelligence, particularly in image caption generation. This project explores nuanced image description synthesis through contextual embeddings, bridging the semantic gap between visual content and linguistic representation. Motivated by the imperative to convey comprehensive information inherent within images, the project investigates the intricate interplay between visual cues and linguistic contexts to generate rich, contextually relevant captions. Deep learning techniques, specifically transfer learning with the inceptionV3 model, are employed for efficient image feature extraction. Concurrently, a pre-trained Glove model maps words into a 200-dimensional vector space in a dedicated embedding layer. Caption generation utilizes Greedy Search and Beam Search methodologies, facilitating the selection of optimal words for precise image description. The foundational resource for this project is the Flickr_8k dataset, enabling the exploration and evaluation of image captioning methods within a diverse and extensive collection of images.

Objectives:
● Develop a robust CNN-LSTM architecture leveraging transfer learning with the inceptionV3 model for comprehensive image feature extraction.

● Investigate the effectiveness of Glove-based word embeddings in capturing nuanced textual representations for image captions.

● Evaluate the performance and contextual relevance of generated image captions using Greedy Search and Beam Search methodologies against established benchmarks.

● Explore the potential application of image caption generation, particularly in enhancing accessibility for visually impaired individuals, thereby promoting inclusivity through technological advancements.
